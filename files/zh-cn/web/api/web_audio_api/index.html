---
title: Web Audio API
slug: Web/API/Web_Audio_API
tags:
  - HTML5音频
  - Web Audio API
translation_of: Web/API/Web_Audio_API
---
<div>
<p>Web Audio API 提供了在Web上控制音频的一个非常有效通用的系统，允许开发者来自选音频源，对音频添加特效，使音频可视化，添加空间效果 （如平移），等等。</p>
</div>

<h2 id="Web_audio_概念与使用">Web audio 概念与使用</h2>

<p>Web Audio API使用户可以在<strong>音频上下文</strong>(AudioContext)中进行音频操作，具有<strong>模块化路由</strong>的特点。在<strong>音频节点</strong>上操作进行基础的音频， 它们连接在一起构成<strong>音频路由图</strong>。即使在单个上下文中也支持多源，尽管这些音频源具有多种不同类型通道布局。这种模块化设计提供了灵活创建动态效果的复合音频的方法。</p>

<p>音频节点通过它们的输入输出相互连接，形成一个链或者一个简单的网。一般来说，这个链或网起始于一个或多个音频源。音频源可以提供一个片段一个片段的音频采样数据（以数组的方式），一般，一秒钟的音频数据可以被切分成几万个这样的片段。这些片段可以是经过一些数学运算得到 （比如<a href="/zh-CN/docs/Web/API/OscillatorNode" title="OscillatorNode 接口表示一个振荡器，它产生一个周期的波形信号（如正弦波）。它是一个 AudioScheduledSourceNode 音频处理模块， 这个模块会生成一个指定频率的波形信号（即一个固定的音调）"><code>OscillatorNode</code></a>），也可以是音频或视频的文件读出来的（比如<a href="/zh-CN/docs/Web/API/AudioBufferSourceNode" title="AudioBufferSourceNode 接口继承自,表现为一个音频源，它包含了一些写在内存中的音频数据，通常储存在一个ArrayBuffer对象中。在处理有严格的时间精确度要求的回放的情形下它尤其有用。比如播放那些需要满足一个指定节奏的声音或者那些储存在内存而不是硬盘或者来自网络的声音。为了播放那些有时间精确度需求但来自网络的流文件或者来自硬盘，则使用"><code>AudioBufferSourceNode</code></a>和<a href="/zh-CN/docs/Web/API/MediaElementAudioSourceNode" title="MediaElementSourceNode 没有输入,只有一个输出,其由使用AudioContext.createMediaElementSource方法创建.输出的频道数目与节点创建时引用音频 HTMLMediaElement  的频道数目一致,或当 HTMLMediaElement 无音频时,频道数目为 1."><code>MediaElementAudioSourceNode</code></a>），又或者是音频流（<a href="/zh-CN/docs/Web/API/MediaStreamAudioSourceNode" title="MediaElementSourceNode没有输入，并且只有一个输出。创建之后使用 AudioContext.createMediaStreamSource方法。输出通道的数量和AudioMediaStreamTrack的通道数量相同。如果没有有效的媒体流，输出通道就变成一个静音的通道。"><code>MediaStreamAudioSourceNode</code></a>）。其实，音频文件本身就是声音的采样数据，这些采样数据可以来自麦克风，也可以来自电子乐器，然后混合成一个单一的复杂的波形。</p>

<p>这些节点的输出可以连接到其它节点的输入上，然后新节点可以对接收到的采样数据再进行其它的处理，再形成一个结果流。一个最常见的操作是通过把输入的采样数据放大来达到扩音器的作用（缩小就是一个弱音器）（参见<a href="/zh-CN/docs/Web/API/GainNode" title><code>GainNode</code></a>）。声音处理完成之后，可以连接到一个目的地（<a href="/zh-CN/docs/Web/API/AudioContext/destination" title="An AudioDestinationNode."><code>AudioContext.destination</code></a>），这个目的地负责把声音数据传输给扬声器或者耳机。注意，只有当用户期望听到声音时，才需要进行最后一个这个连接。</p>

<p>一个简单而典型的web audio流程如下：</p>

<ol>
 <li>创建音频上下文</li>
 <li>在音频上下文里创建源 — 例如 <code>&lt;audio&gt;</code>, 振荡器, 流</li>
 <li>创建效果节点，例如混响、双二阶滤波器、平移、压缩</li>
 <li>为音频选择一个目的地，例如你的系统扬声器</li>
 <li>连接源到效果器，对目的地进行效果输出</li>
</ol>

<p><img alt="A simple box diagram with an outer box labeled Audio context, and three inner boxes labeled Sources, Effects and Destination. The three inner boxes have arrow between them pointing from left to right, indicating the flow of audio information." src="https://mdn.mozillademos.org/files/12241/webaudioAPI_en.svg"></p>

<p>使用这个API，时间可以被非常精确地控制，几乎没有延迟，这样开发人员可以准确地响应事件，并且可以针对采样数据进行编程，甚至是较高的采样率。这样，鼓点和节拍是准确无误的。</p>

<p>Web Audio API 也使我们能够控制音频的<em>空间化</em>。在基于<em>源 - 侦听器模型</em>的系统中，它允许控制<em>平移模型</em>和处理<em>距离引起的衰减</em>或移动源（移动侦听）引起的<em>多普勒效应</em>。</p>

<div class="note">
<p><strong>注意</strong>: 你可以阅读我们关于 Web Audio API 的文章来了解更多细节 <a href="/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API">Web Audio API背后的基本概念</a>。</p>
</div>

<h2 id="Web_Audio_API_接口">Web Audio API 接口</h2>

<p>Web Audio API共有一系列接口和相关的事件，我们已经把它们分成了九类功能。</p>

<h3 id="通用音频图定义">通用音频图定义</h3>

<p>Web Audio API 中与生成音频图相关的定义与通用容器。</p>

<dl>
 <dt><a href="/zh-CN/docs/Web/API/AudioContext" title="AudioContext接口表示由音频模块连接而成的音频处理图，每个模块对应一个AudioNode。AudioContext可以控制它所包含的节点的创建，以及音频处理、解码操作的执行。做任何事情之前都要先创建AudioContext对象，因为一切都发生在这个环境之中。"><code>AudioContext</code></a></dt>
 <dd><strong><code>AudioContext</code></strong>接口代表由音频模块构成的音频处理图。音频上下文控制其所包含节点的创建和音频处理、解码。使用其它接口前你必需创建一个<code>音频上下文</code>，一切操作都在这个环境里进行。</dd>
 <dt><a href="/zh-CN/docs/Web/API/AudioNode" title="AudioNode 接口是一个处理音频的通用模块, 比如一个音频源 (e.g. 一个 HTML &lt;audio&gt; or &lt;video&gt; 元素), 一个音频地址或者一个中间处理模块 (e.g. 一个过滤器如 BiquadFilterNode, 或一个音量控制器如 GainNode)."><code>AudioNode</code></a></dt>
 <dd><strong><code>音频节点</code></strong><strong> </strong>接口是一个音频处理模块， 例如音频源（<a href="/zh-CN/docs/Web/HTML/Element/audio" title="HTML &lt;audio&gt; 元素用于在文档中表示音频内容。 &lt;audio&gt; 元素可以包含多个音频资源， 这些音频资源可以使用 src 属性或者&lt;source&gt; 元素来进行描述； 浏览器将会选择最合适的一个来使用。对于不支持&lt;audio&gt;元素的浏览器，&lt;audio&gt;元素也可以作为浏览器不识别的内容加入到文档中。"><code>&lt;audio&gt;</code></a>或<a href="/zh-CN/docs/Web/HTML/Element/video" title="HTML &lt;video&gt; 元素 用于在HTML或者XHTML文档中嵌入媒体播放器，用于支持文档内的视频播放。"><code>&lt;video&gt;</code></a>），音频输出、中间处理模块（例如：滤波器 <a href="/zh-CN/docs/Web/API/BiquadFilterNode" title="BiquadFilterNode接口表示一个简单低阶滤波器(双二阶滤波器), 通过 AudioContext.createBiquadFilter() 方法创建. 它是一个能表示不同类型的过滤器，声调控制设备，图形均衡器的AudioNode ."><code>BiquadFilterNode</code></a> 或者音量控制器 <a href="/zh-CN/docs/Web/API/GainNode" title><code>GainNode</code></a>）。</dd>
 <dt><a href="/zh-CN/docs/Web/API/AudioParam" title="下面有两种类型的 AudioParam, a-rate 和 k-rate 参数："><code>AudioParam</code></a></dt>
 <dd><strong><code>AudioParam</code></strong><strong> </strong>接口代表音频相关的参数，比如一个 <a href="/zh-CN/docs/Web/API/AudioNode" title="AudioNode 接口是一个处理音频的通用模块, 比如一个音频源 (e.g. 一个 HTML &lt;audio&gt; or &lt;video&gt; 元素), 一个音频地址或者一个中间处理模块 (e.g. 一个过滤器如 BiquadFilterNode, 或一个音量控制器如 GainNode)."><code>AudioNode</code></a>的参数。它可以设置为特定值或值的变化，并且可以在指定的时间之后以指定模式变更。</dd>
 <dt><code><a href="/zh-CN/docs/Web/Reference/Events/ended" title="/zh-CN/docs/Web/Reference/Events/ended">ended</a></code>结束事件</dt>
 <dd>当媒体播放停止时，会触发<code>ended</code>事件。</dd>
</dl>

<h3 id="定义音频源">定义音频源</h3>

<p>Web Audio API 使用的音频源接口。</p>

<dl>
 <dt><a href="/zh-CN/docs/Web/API/OscillatorNode" title="OscillatorNode 接口表示一个振荡器，它产生一个周期的波形信号（如正弦波）。它是一个 AudioScheduledSourceNode 音频处理模块， 这个模块会生成一个指定频率的波形信号（即一个固定的音调）"><code>OscillatorNode</code></a></dt>
 <dd><strong><code style="font-size: 14px;">OscillatorNode</code></strong>接口代表一种随时间变化的波形，比如正弦波形或三角波形。类型是<a href="/zh-CN/docs/Web/API/AudioNode" title="AudioNode 接口是一个处理音频的通用模块, 比如一个音频源 (e.g. 一个 HTML &lt;audio&gt; or &lt;video&gt; 元素), 一个音频地址或者一个中间处理模块 (e.g. 一个过滤器如 BiquadFilterNode, 或一个音量控制器如 GainNode)."><code>AudioNode</code></a>，功能是音频处理模块，可以产生指定<em>频率</em>的波形。</dd>
 <dt><a href="/zh-CN/docs/Web/API/AudioBuffer" title="这些类型对象被设计来控制小音频片段，往往短于45秒。对于更长的声音，通过 MediaElementAudioSourceNode来实现更为合适。缓存区（buffer）包含以下数据：不间断的IEEE75432位线性PCM，从-1到1的范围额定，就是说，32位的浮点缓存区的每个样本在-1.0到1.0之间。如果AudioBuffer有不同的频道，他们通常被保存在独立的缓存区。"><code>AudioBuffer</code></a></dt>
 <dd><strong><code>AudioBuffer</code></strong>代表内存中的一段音频数据，可以通过<a href="/zh-CN/docs/Web/API/AudioContext/decodeAudioData" title="这是从音频轨道创建用于web audio API音频源的首选方法。"><code>AudioContext.decodeAudioData()</code></a>方法从音频文件创建，也可以通过<a href="/zh-CN/docs/Web/API/AudioContext/createBuffer" title="音频环境AudioContext 接口的 createBuffer() 方法用于新建一个空白的 AudioBuffer 对象，以便用于填充数据，通过 AudioBufferSourceNode 播放。"><code>AudioContext.createBuffer()</code></a>方法从原始数据创建。当音频数据被解码成这种格式之后，就可以被放入一个<a href="/zh-CN/docs/Web/API/AudioBufferSourceNode" title="AudioBufferSourceNode 接口继承自,表现为一个音频源，它包含了一些写在内存中的音频数据，通常储存在一个ArrayBuffer对象中。在处理有严格的时间精确度要求的回放的情形下它尤其有用。比如播放那些需要满足一个指定节奏的声音或者那些储存在内存而不是硬盘或者来自网络的声音。为了播放那些有时间精确度需求但来自网络的流文件或者来自硬盘，则使用"><code>AudioBufferSourceNode</code></a>中使用。</dd>
 <dt><a href="/zh-CN/docs/Web/API/AudioBufferSourceNode" title="AudioBufferSourceNode 接口继承自,表现为一个音频源，它包含了一些写在内存中的音频数据，通常储存在一个ArrayBuffer对象中。在处理有严格的时间精确度要求的回放的情形下它尤其有用。比如播放那些需要满足一个指定节奏的声音或者那些储存在内存而不是硬盘或者来自网络的声音。为了播放那些有时间精确度需求但来自网络的流文件或者来自硬盘，则使用"><code>AudioBufferSourceNode</code></a></dt>
 <dd><strong><code>AudioBufferSourceNode</code></strong>表示由内存音频数据组成的音频源，音频数据存储在<a href="/zh-CN/docs/Web/API/AudioBuffer" title="这些类型对象被设计来控制小音频片段，往往短于45秒。对于更长的声音，通过 MediaElementAudioSourceNode来实现更为合适。缓存区（buffer）包含以下数据：不间断的IEEE75432位线性PCM，从-1到1的范围额定，就是说，32位的浮点缓存区的每个样本在-1.0到1.0之间。如果AudioBuffer有不同的频道，他们通常被保存在独立的缓存区。"><code>AudioBuffer</code></a>中。这是一个作为音频源的<a href="/zh-CN/docs/Web/API/AudioNode" title="AudioNode 接口是一个处理音频的通用模块, 比如一个音频源 (e.g. 一个 HTML &lt;audio&gt; or &lt;video&gt; 元素), 一个音频地址或者一个中间处理模块 (e.g. 一个过滤器如 BiquadFilterNode, 或一个音量控制器如 GainNode)."><code>AudioNode</code></a>。</dd>
 <dt><a href="/zh-CN/docs/Web/API/MediaElementAudioSourceNode" title="MediaElementSourceNode 没有输入,只有一个输出,其由使用AudioContext.createMediaElementSource方法创建.输出的频道数目与节点创建时引用音频 HTMLMediaElement  的频道数目一致,或当 HTMLMediaElement 无音频时,频道数目为 1."><code>MediaElementAudioSourceNode</code></a></dt>
 <dd><code><strong>MediaElementAudio</strong></code><strong><code>SourceNode</code></strong>接口表示由HTML5 <a href="/zh-CN/docs/Web/HTML/Element/audio" title="HTML &lt;audio&gt; 元素用于在文档中表示音频内容。 &lt;audio&gt; 元素可以包含多个音频资源， 这些音频资源可以使用 src 属性或者&lt;source&gt; 元素来进行描述； 浏览器将会选择最合适的一个来使用。对于不支持&lt;audio&gt;元素的浏览器，&lt;audio&gt;元素也可以作为浏览器不识别的内容加入到文档中。"><code>&lt;audio&gt;</code></a>或<a href="/zh-CN/docs/Web/HTML/Element/video" title="HTML &lt;video&gt; 元素 用于在HTML或者XHTML文档中嵌入媒体播放器，用于支持文档内的视频播放。"><code>&lt;video&gt;</code></a>元素生成的音频源。这是一个作为音频源的<a href="/zh-CN/docs/Web/API/AudioNode" title="AudioNode 接口是一个处理音频的通用模块, 比如一个音频源 (e.g. 一个 HTML &lt;audio&gt; or &lt;video&gt; 元素), 一个音频地址或者一个中间处理模块 (e.g. 一个过滤器如 BiquadFilterNode, 或一个音量控制器如 GainNode)."><code>AudioNode</code></a>。</dd>
 <dt><a href="/zh-CN/docs/Web/API/MediaStreamAudioSourceNode" title="MediaElementSourceNode没有输入，并且只有一个输出。创建之后使用 AudioContext.createMediaStreamSource方法。输出通道的数量和AudioMediaStreamTrack的通道数量相同。如果没有有效的媒体流，输出通道就变成一个静音的通道。"><code>MediaStreamAudioSourceNode</code></a></dt>
 <dd><code><strong>MediaStreamAudio</strong></code><strong><code>SourceNode</code></strong>接口表示由 WebRTC <a href="/zh-CN/docs/Web/API/MediaStream" title="MediaStream 接口是一个媒体内容的流.。一个流包含几个轨道，比如视频和音频轨道。"><code>MediaStream</code></a>（如网络摄像头或麦克风）生成的音频源。这是一个作为音频源的<a href="/zh-CN/docs/Web/API/AudioNode" title="AudioNode 接口是一个处理音频的通用模块, 比如一个音频源 (e.g. 一个 HTML &lt;audio&gt; or &lt;video&gt; 元素), 一个音频地址或者一个中间处理模块 (e.g. 一个过滤器如 BiquadFilterNode, 或一个音量控制器如 GainNode)."><code>AudioNode</code></a>。</dd>
</dl>

<h3 id="定义音效">定义音效</h3>

<p>应用到音频源上的音效。</p>

<dl>
 <dt><a href="/zh-CN/docs/Web/API/BiquadFilterNode" title="BiquadFilterNode接口表示一个简单低阶滤波器(双二阶滤波器), 通过 AudioContext.createBiquadFilter() 方法创建. 它是一个能表示不同类型的过滤器，声调控制设备，图形均衡器的AudioNode ."><code>BiquadFilterNode</code></a></dt>
 <dd><strong><code>BiquadFilterNode</code></strong><strong> </strong>接口表示一个简单的低频滤波器。它是一个<a href="/zh-CN/docs/Web/API/AudioNode" title="AudioNode 接口是一个处理音频的通用模块, 比如一个音频源 (e.g. 一个 HTML &lt;audio&gt; or &lt;video&gt; 元素), 一个音频地址或者一个中间处理模块 (e.g. 一个过滤器如 BiquadFilterNode, 或一个音量控制器如 GainNode)."><code>AudioNode</code></a>，可以表示不同种类的滤波器、调音器或图形均衡器。<code>BiquadFilterNode</code> 总是只有一个输入和一个输出。</dd>
 <dt><a href="/zh-CN/docs/Web/API/ConvolverNode" title="此页面仍未被本地化, 期待您的翻译!"><code>ConvolverNode</code></a></dt>
 <dd><code><strong>Convolver</strong></code><strong><code>Node</code></strong><strong> </strong>接口是一个<span style="line-height: 1.5;"><a href="/zh-CN/docs/Web/API/AudioNode" title="AudioNode 接口是一个处理音频的通用模块, 比如一个音频源 (e.g. 一个 HTML &lt;audio&gt; or &lt;video&gt; 元素), 一个音频地址或者一个中间处理模块 (e.g. 一个过滤器如 BiquadFilterNode, 或一个音量控制器如 GainNode)."><code>AudioNode</code></a></span>，<span style="line-height: 1.5;">对给定的 AudioBuffer 执行线性卷积，通常用于实现混响效果</span><span style="line-height: 1.5;">。</span></dd>
 <dt><a href="/zh-CN/docs/Web/API/DelayNode" title="此页面仍未被本地化, 期待您的翻译!"><code>DelayNode</code></a></dt>
 <dd><strong><code>DelayNode</code></strong><strong> </strong>接口表示<a href="http://en.wikipedia.org/wiki/Digital_delay_line" title="http://en.wikipedia.org/wiki/Digital_delay_line">延迟线</a>；是<a href="/zh-CN/docs/Web/API/AudioNode" title="AudioNode 接口是一个处理音频的通用模块, 比如一个音频源 (e.g. 一个 HTML &lt;audio&gt; or &lt;video&gt; 元素), 一个音频地址或者一个中间处理模块 (e.g. 一个过滤器如 BiquadFilterNode, 或一个音量控制器如 GainNode)."><code>AudioNode</code></a> 类型的音频处理模块，使输入的数据延时输出。</dd>
 <dt><a href="/zh-CN/docs/Web/API/DynamicsCompressorNode" title="由父类 AudioNode 派生"><code>DynamicsCompressorNode</code></a></dt>
 <dd><strong><code>DynamicsCompressorNode</code></strong> 提供了一个压缩效果，当多个音频在同时播放并且混合的时候，可以通过它降低音量最大的部分的音量来帮助避免发生削波和失真。</dd>
 <dt><a href="/zh-CN/docs/Web/API/GainNode" title><code>GainNode</code></a></dt>
 <dd><strong><code>GainNode</code></strong><strong> </strong>接口用于音量变化。它是一个 <a href="/zh-CN/docs/Web/API/AudioNode" title="AudioNode 接口是一个处理音频的通用模块, 比如一个音频源 (e.g. 一个 HTML &lt;audio&gt; or &lt;video&gt; 元素), 一个音频地址或者一个中间处理模块 (e.g. 一个过滤器如 BiquadFilterNode, 或一个音量控制器如 GainNode)."><code>AudioNode</code></a> 类型的音频处理模块，输入后应用<em>增益</em> 效果，然后输出。</dd>
 <dt><a href="/zh-CN/docs/Web/API/StereoPannerNode" title="此页面仍未被本地化, 期待您的翻译!"><code>StereoPannerNode</code></a></dt>
 <dd><code><strong>StereoPannerNode</strong></code> 接口表示一个简单立体声控制节点，用来左右移动音频流。</dd>
 <dt><a href="/zh-CN/docs/Web/API/WaveShaperNode" title="一个WaveShaperNode 总是有一个确切的输入和输出。"><code>WaveShaperNode</code></a></dt>
 <dd><strong><code>WaveShaperNode</code></strong>接口表示一个非线性的扭曲。它是<a href="/zh-CN/docs/Web/API/AudioNode" title="AudioNode 接口是一个处理音频的通用模块, 比如一个音频源 (e.g. 一个 HTML &lt;audio&gt; or &lt;video&gt; 元素), 一个音频地址或者一个中间处理模块 (e.g. 一个过滤器如 BiquadFilterNode, 或一个音量控制器如 GainNode)."><code>AudioNode</code></a>类型，可以利用曲线来对信号进行扭曲。除了一些效果明显的扭曲，还常被用来给声音添加温暖的感觉。</dd>
 <dt><a href="/zh-CN/docs/Web/API/PeriodicWave" title="PeriodicWave (周期波) 没有输入或输出；它用于调用 OscillatorNode.setPeriodicWave() 时定义自定义振荡器。 PeriodicWave 自身由 AudioContext.createPeriodicWave() 创建/返回。"><code>PeriodicWave</code></a></dt>
 <dd>用来定义周期性的波形，可被用来重塑 <a href="/zh-CN/docs/Web/API/OscillatorNode" title="OscillatorNode 接口表示一个振荡器，它产生一个周期的波形信号（如正弦波）。它是一个 AudioScheduledSourceNode 音频处理模块， 这个模块会生成一个指定频率的波形信号（即一个固定的音调）"><code>OscillatorNode</code></a>的输出.</dd>
</dl>

<h3 id="定义音频目的地">定义音频目的地</h3>

<p>在你处理完音频之后，这些接口定义了输出到哪里。</p>

<dl>
 <dt><a href="/zh-CN/docs/Web/API/AudioDestinationNode" title="AudioDestinationNode可以通过AudioContext.destination属性来查看。"><code>AudioDestinationNode</code></a></dt>
 <dd><strong><code>AudioDestinationNode</code></strong> 定义了最后音频要输出到哪里，通常是输出到你的扬声器。</dd>
 <dt><a href="/zh-CN/docs/Web/API/MediaStreamAudioDestinationNode" title="此页面仍未被本地化, 期待您的翻译!"><code>MediaStreamAudioDestinationNode</code></a></dt>
 <dd><code><strong>MediaStreamAudio</strong></code><strong><code>DestinationNode</code></strong>定义了使用 <a href="https://developer.mozilla.org/en-US/docs/WebRTC" title="/en-US/docs/WebRTC">WebRTC</a> 的<a href="/zh-CN/docs/Web/API/MediaStream" title="MediaStream 接口是一个媒体内容的流.。一个流包含几个轨道，比如视频和音频轨道。"><code>MediaStream</code></a> （只包含单个AudioMediaStreamTrack）应该连接的目的地，AudioMediaStreamTrack的使用方式和从<a href="/zh-CN/docs/Web/API/MediaDevices/getUserMedia" title="通常你可以使用 navigator.mediaDevices 来获取 MediaDevices ，例如："><code>getUserMedia()</code></a>中得到<a href="/zh-CN/docs/Web/API/MediaStream" title="MediaStream 接口是一个媒体内容的流.。一个流包含几个轨道，比如视频和音频轨道。"><code>MediaStream</code></a>相似。这个接口是<a href="/zh-CN/docs/Web/API/AudioNode" title="AudioNode 接口是一个处理音频的通用模块, 比如一个音频源 (e.g. 一个 HTML &lt;audio&gt; or &lt;video&gt; 元素), 一个音频地址或者一个中间处理模块 (e.g. 一个过滤器如 BiquadFilterNode, 或一个音量控制器如 GainNode)."><code>AudioNode</code></a>类型的音频目的地。</dd>
</dl>

<h3 id="数据分析和可视化">数据分析和可视化</h3>

<p>如果你想从音频里提取时间、频率或者其它数据，你需要 AnalyserNode。</p>

<dl>
 <dt><a href="/zh-CN/docs/Web/API/AnalyserNode" title="AnalyserNode 接口表示了一个可以提供实时频域和时域分析信息的节点。它是一个不对音频流作任何改动的 AudioNode，同时允许你获取和处理它生成的数据，从而创建音频可视化。"><code>AnalyserNode</code></a></dt>
 <dd><strong><code>AnalyserNode</code></strong>表示一个可以提供实时频率分析与时域分析的切点，这些分析数据可以用做数据分析和可视化。</dd>
</dl>

<h3 id="分离、合并声道">分离、合并声道</h3>

<p>你将使用这些接口来拆分、合并声道。</p>

<dl>
 <dt><a href="/zh-CN/docs/Web/API/ChannelSplitterNode" title="此页面仍未被本地化, 期待您的翻译!"><code>ChannelSplitterNode</code></a></dt>
 <dd><code><strong>ChannelSplitterNode</strong></code>可以把输入流的每个声道输出到一个独立的输出流。</dd>
 <dt><a href="/zh-CN/docs/Web/API/ChannelMergerNode" title="ChannelMergerNode 接口，经常与其对应的 ChannelSplitterNode 接口一起使用，将不同的单声道输入重新组合成单个输出。每个输入用于填充输出的一个通道。这对于分别访问每个通道非常有用，例如，执行通道混合时，必须在每个信道上单独控制增益。"><code>ChannelMergerNode</code></a></dt>
 <dd><code><strong>ChannelMergerNode</strong></code>用于把一组输入流合成到一个输出流。输出流的每一个声道对应一个输入流。</dd>
</dl>

<h3 id="声音空间效果">声音空间效果</h3>

<p>这些接口用来添加空间平移效果到音频源。</p>

<dl>
 <dt><a href="/zh-CN/docs/Web/API/AudioListener" title="此页面仍未被本地化, 期待您的翻译!"><code>AudioListener</code></a></dt>
 <dd><strong><code>AudioListener</code></strong>代表场景中正在听声音的人的位置和朝向。</dd>
 <dt><a href="/zh-CN/docs/Web/API/PannerNode" title="此页面仍未被本地化, 期待您的翻译!"><code>PannerNode</code></a></dt>
 <dd><strong><code>PannerNode</code></strong>用于表示场景是声音的空间行为。它是<a href="/zh-CN/docs/Web/API/AudioNode" title="AudioNode 接口是一个处理音频的通用模块, 比如一个音频源 (e.g. 一个 HTML &lt;audio&gt; or &lt;video&gt; 元素), 一个音频地址或者一个中间处理模块 (e.g. 一个过滤器如 BiquadFilterNode, 或一个音量控制器如 GainNode)."><code>AudioNode</code></a>类型的音频处理模块，这个节点用于表示右手笛卡尔坐标系里声源的位置信息，运动信息（通过一个速度向量表示），方向信息（通过一个方向圆锥表示）。</dd>
</dl>

<h3 id="使用_JavaScript_处理音频">使用 JavaScript 处理音频</h3>

<p>可以编写JavaScript代码来处理音频数据。当然，这需要用到下面的接口和事件。</p>

<div class="note">
<p><strong>注意：</strong>这些功能在Web Audio API的2014年8月9日版本中已经标记为不推荐的，这些功能很快会被<a href="#Audio_Workers">Audio_Workers</a>代替。</p>
</div>

<dl>
 <dt><a href="/zh-CN/docs/Web/API/ScriptProcessorNode" title><code>ScriptProcessorNode</code></a></dt>
 <dd><strong><code>ScriptProcessorNode</code></strong>接口用于通过JavaScript代码生成，处理，分析音频。它是一个<a href="/zh-CN/docs/Web/API/AudioNode" title="AudioNode 接口是一个处理音频的通用模块, 比如一个音频源 (e.g. 一个 HTML &lt;audio&gt; or &lt;video&gt; 元素), 一个音频地址或者一个中间处理模块 (e.g. 一个过滤器如 BiquadFilterNode, 或一个音量控制器如 GainNode)."><code>AudioNode</code></a>类型的音频处理模块，但是它与两个缓冲区相连接，一个缓冲区里包含当前的输入数据，另一个缓冲区里包含着输出数据。每当新的音频数据被放入输入缓冲区，就会产生一个<a href="/zh-CN/docs/Web/API/AudioProcessingEvent" title="此页面仍未被本地化, 期待您的翻译!"><code>AudioProcessingEvent</code></a>事件，当这个事件处理结束时，输出缓冲区里应该写好了新数据。</dd>
 <dt><code><a href="/zh-CN/docs/Web/Reference/Events/audioprocess" title="/zh-CN/docs/Web/Reference/Events/audioprocess">audioprocess</a></code> (event)</dt>
 <dd>当一个Web Audio API <a href="/zh-CN/docs/Web/API/ScriptProcessorNode" title><code>ScriptProcessorNode</code></a>已经准备好进行处理时，这个事件回调会被调用。</dd>
 <dt><a href="/zh-CN/docs/Web/API/AudioProcessingEvent" title="此页面仍未被本地化, 期待您的翻译!"><code>AudioProcessingEvent</code></a></dt>
 <dd>当<a href="/zh-CN/docs/Web/API/ScriptProcessorNode" title><code>ScriptProcessorNode</code></a>的输入流数据准备好了时，<code>AudioProcessingEvent</code>事件会产生。</dd>
</dl>

<h3 id="离线（后台）音频处理">离线（后台）音频处理</h3>

<p>在后台进行音频的快速处理也是可以的。仅仅生成包含音频数据的<a href="/zh-CN/docs/Web/API/AudioBuffer" title="这些类型对象被设计来控制小音频片段，往往短于45秒。对于更长的声音，通过 MediaElementAudioSourceNode来实现更为合适。缓存区（buffer）包含以下数据：不间断的IEEE75432位线性PCM，从-1到1的范围额定，就是说，32位的浮点缓存区的每个样本在-1.0到1.0之间。如果AudioBuffer有不同的频道，他们通常被保存在独立的缓存区。"><code>AudioBuffer</code></a>，而不在扬声器里播放它。</p>

<dl>
 <dt><a href="/zh-CN/docs/Web/API/OfflineAudioContext" title="OfflineAudioContext 接口是一个 AudioContext 的接口，代表由多个 AudioNode 连接在一起构成的音频处理图。与 AudioContext 标准相反的是， OfflineAudioContext 不在硬件设备渲染音频；相反，它尽可能快地生成音频，输出一个 AudioBuffer 作为结果。"><code>OfflineAudioContext</code></a></dt>
 <dd><strong><code>OfflineAudioContext</code></strong>离线音频上下文也是音频上下文<a href="/zh-CN/docs/Web/API/AudioContext" title="AudioContext接口表示由音频模块连接而成的音频处理图，每个模块对应一个AudioNode。AudioContext可以控制它所包含的节点的创建，以及音频处理、解码操作的执行。做任何事情之前都要先创建AudioContext对象，因为一切都发生在这个环境之中。"><code>AudioContext</code></a>，也表示把<a href="/zh-CN/docs/Web/API/AudioNode" title="AudioNode 接口是一个处理音频的通用模块, 比如一个音频源 (e.g. 一个 HTML &lt;audio&gt; or &lt;video&gt; 元素), 一个音频地址或者一个中间处理模块 (e.g. 一个过滤器如 BiquadFilterNode, 或一个音量控制器如 GainNode)."><code>AudioNode</code></a>连接到一起的一个音频处理图。但是，与一个标准的音频上下文相比，离线上下文不能把音频渲染到扬声器，仅仅是把音频渲染到一个缓冲区。</dd>
 <dt><code><a href="/zh-CN/docs/Web/Reference/Events/complete" title="/zh-CN/docs/Web/Reference/Events/complete">complete</a></code> (event)</dt>
 <dd>Complete事件，当离线音频上下文被终止时产生。</dd>
 <dt><a href="/zh-CN/docs/Web/API/OfflineAudioCompletionEvent" title="此页面仍未被本地化, 期待您的翻译!"><code>OfflineAudioCompletionEvent</code></a></dt>
 <dd><code>OfflineAudioCompletionEvent</code>表示上下文被终止时的事件。</dd>
</dl>

<h3 id="Audio_Workers" name="Audio_Workers">音频工作者</h3>

<p>在了解这一部分内容之前，你可以先了解一个新的WebWorker方面的内容。音频工作者提供了一种可以在一个<a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API/Using_web_workers">WebWorker</a>上下文中直接进行音频处理的方式。现在已经定义了一些这部分功能的新接口，接口定义是在2014年的8月29日文档中。到目前为止，还没有浏览器已经对这些接口进行了实现。当这些接口被实现后，<a href="/zh-CN/docs/Web/API/ScriptProcessorNode" title><code>ScriptProcessorNode</code></a>和<a href="https://developer.mozilla.org/zh-CN/docs/Web/API/Web_Audio_API$edit#Audio_processing_via_JavaScript">前文</a>中提到的其它接口都会被替代。</p>

<dl>
 <dt><a href="/zh-CN/docs/Web/API/AudioWorkerNode" title="此页面仍未被本地化, 期待您的翻译!"><code>AudioWorkerNode</code></a></dt>
 <dd>AudioWorkerNode也是<a href="/zh-CN/docs/Web/API/AudioNode" title="AudioNode 接口是一个处理音频的通用模块, 比如一个音频源 (e.g. 一个 HTML &lt;audio&gt; or &lt;video&gt; 元素), 一个音频地址或者一个中间处理模块 (e.g. 一个过滤器如 BiquadFilterNode, 或一个音量控制器如 GainNode)."><code>AudioNode</code></a>类型，但是它用于与工作者线程合作来直接完成音频的生成，处理或分析等操作。</dd>
 <dt><a href="/zh-CN/docs/Web/API/AudioWorkerGlobalScope" title="此页面仍未被本地化, 期待您的翻译!"><code>AudioWorkerGlobalScope</code></a></dt>
 <dd><code>AudioWorkerGlobalScope<font face="Open Sans, arial, sans-serif">继承于</font></code><code>DedicatedWorkerGlobalScope</code>。代表一个工作者上下文。这个工作者上下文里运行着对音频进行处理的脚本。设计这个接口的目的，是为了直接通过编写JavaScript代码，来完成对音频数据的生成，处理，分析工作。</dd>
 <dt><a href="/zh-CN/docs/Web/API/AudioProcessEvent" title="此页面仍未被本地化, 期待您的翻译!"><code>AudioProcessEvent</code></a></dt>
 <dd>这是一个事件对象。这个对象会被分发给<a href="/zh-CN/docs/Web/API/AudioWorkerGlobalScope" title="此页面仍未被本地化, 期待您的翻译!"><code>AudioWorkerGlobalScope</code></a>对象来进行处理。</dd>
</dl>

<h2 id="Example" name="Example">过时的接口</h2>

<p>下面介绍到的这些接口定义于比较老的Web Audio API标准，现在已经过时了，而且已经被新接口取代。</p>

<dl>
 <dt><a href="/zh-CN/docs/Web/API/JavaScriptNode" title="此页面仍未被本地化, 期待您的翻译!"><code>JavaScriptNode</code></a></dt>
 <dd>用于通过编写JavaScript代码直接处理音频数据，现在已经被<a href="/zh-CN/docs/Web/API/ScriptProcessorNode" title><code>ScriptProcessorNode</code></a>取代。</dd>
 <dt><a href="/zh-CN/docs/Web/API/WaveTableNode" title="此页面仍未被本地化, 期待您的翻译!"><code>WaveTableNode</code></a></dt>
 <dd>用于定义一个周期性波形，现在已经被<a href="/zh-CN/docs/Web/API/PeriodicWave" title="PeriodicWave (周期波) 没有输入或输出；它用于调用 OscillatorNode.setPeriodicWave() 时定义自定义振荡器。 PeriodicWave 自身由 AudioContext.createPeriodicWave() 创建/返回。"><code>PeriodicWave</code></a>取代。</dd>
</dl>

<h2 id="Example" name="Example">示例</h2>

<p>下面的这个示例使用了较多的Web Audio API接口，可以通过访问<a href="https://mdn.github.io/voice-change-o-matic/">网址</a>来查看它的时时运行情况，也可以访问GitHub上它的<a href="https://github.com/mdn/voice-change-o-matic">源代码</a>。这个示例里会对声音的音量进行改变，打开页面时，可以先把扬声器的音量调小一些。</p>

<p>Web Audio API接口在下面的代码里已经高亮显示。</p>

<pre class="brush: js; highlight:[1,2,9,10,11,12,36,37,38,39,40,41,62,63,72,114,115,121,123,124,125,147,151]">var audioCtx = new (window.AudioContext || window.webkitAudioContext)(); // define audio context
// Webkit/blink browsers need prefix, Safari won&apos;t work without window.

var voiceSelect = document.getElementById(&quot;voice&quot;); // select box for selecting voice effect options
var visualSelect = document.getElementById(&quot;visual&quot;); // select box for selecting audio visualization options
var mute = document.querySelector(&apos;.mute&apos;); // mute button
var drawVisual; // requestAnimationFrame

var analyser = audioCtx.createAnalyser();
var distortion = audioCtx.createWaveShaper();
var gainNode = audioCtx.createGain();
var biquadFilter = audioCtx.createBiquadFilter();

function makeDistortionCurve(amount) { // function to make curve shape for distortion/wave shaper node to use
  var k = typeof amount === &apos;number&apos; ? amount : 50,
    n_samples = 44100,
    curve = new Float32Array(n_samples),
    deg = Math.PI / 180,
    i = 0,
    x;
  for ( ; i &lt; n_samples; ++i ) {
    x = i * 2 / n_samples - 1;
    curve[i] = ( 3 + k ) * x * 20 * deg / ( Math.PI + k * Math.abs(x) );
  }
  return curve;
};

navigator.getUserMedia (
  // constraints - only audio needed for this app
  {
    audio: true
  },

  // Success callback
  function(stream) {
    source = audioCtx.createMediaStreamSource(stream);
    source.connect(analyser);
    analyser.connect(distortion);
    distortion.connect(biquadFilter);
    biquadFilter.connect(gainNode);
    gainNode.connect(audioCtx.destination); // connecting the different audio graph nodes together

    visualize(stream);
    voiceChange();

  },

  // Error callback
  function(err) {
    console.log(&apos;The following gUM error occured: &apos; + err);
  }
);

function visualize(stream) {
  WIDTH = canvas.width;
  HEIGHT = canvas.height;

  var visualSetting = visualSelect.value;
  console.log(visualSetting);

  if(visualSetting == &quot;sinewave&quot;) {
    analyser.fftSize = 2048;
    var bufferLength = analyser.frequencyBinCount; // half the FFT value
    var dataArray = new Uint8Array(bufferLength); // create an array to store the data

    canvasCtx.clearRect(0, 0, WIDTH, HEIGHT);

    function draw() {

      drawVisual = requestAnimationFrame(draw);

      analyser.getByteTimeDomainData(dataArray); // get waveform data and put it into the array created above

      canvasCtx.fillStyle = &apos;rgb(200, 200, 200)&apos;; // draw wave with canvas
      canvasCtx.fillRect(0, 0, WIDTH, HEIGHT);

      canvasCtx.lineWidth = 2;
      canvasCtx.strokeStyle = &apos;rgb(0, 0, 0)&apos;;

      canvasCtx.beginPath();

      var sliceWidth = WIDTH * 1.0 / bufferLength;
      var x = 0;

      for(var i = 0; i &lt; bufferLength; i++) {

        var v = dataArray[i] / 128.0;
        var y = v * HEIGHT/2;

        if(i === 0) {
          canvasCtx.moveTo(x, y);
        } else {
          canvasCtx.lineTo(x, y);
        }

        x += sliceWidth;
      }

      canvasCtx.lineTo(canvas.width, canvas.height/2);
      canvasCtx.stroke();
    };

    draw();

  } else if(visualSetting == &quot;off&quot;) {
    canvasCtx.clearRect(0, 0, WIDTH, HEIGHT);
    canvasCtx.fillStyle = &quot;red&quot;;
    canvasCtx.fillRect(0, 0, WIDTH, HEIGHT);
  }

}

function voiceChange() {
  distortion.curve = new Float32Array;
  biquadFilter.gain.value = 0; // reset the effects each time the voiceChange function is run

  var voiceSetting = voiceSelect.value;
  console.log(voiceSetting);

  if(voiceSetting == &quot;distortion&quot;) {
    distortion.curve = makeDistortionCurve(400); // apply distortion to sound using waveshaper node
  } else if(voiceSetting == &quot;biquad&quot;) {
    biquadFilter.type = &quot;lowshelf&quot;;
    biquadFilter.frequency.value = 1000;
    biquadFilter.gain.value = 25; // apply lowshelf filter to sounds using biquad
  } else if(voiceSetting == &quot;off&quot;) {
    console.log(&quot;Voice settings turned off&quot;); // do nothing, as off option was chosen
  }

}

// event listeners to change visualize and voice settings

visualSelect.onchange = function() {
  window.cancelAnimationFrame(drawVisual);
  visualize(stream);
}

voiceSelect.onchange = function() {
  voiceChange();
}

mute.onclick = voiceMute;

function voiceMute() { // toggle to mute and unmute sound
  if(mute.id == &quot;&quot;) {
    gainNode.gain.value = 0; // gain set to 0 to mute sound
    mute.id = &quot;activated&quot;;
    mute.innerHTML = &quot;Unmute&quot;;
  } else {
    gainNode.gain.value = 1; // gain set to 1 to unmute sound
    mute.id = &quot;&quot;;
    mute.innerHTML = &quot;Mute&quot;;
  }
}
</pre>

<h2 id="规范">规范</h2>

<table class="standard-table">
 <tbody>
  <tr>
   <th scope="col">Specification</th>
   <th scope="col">Status</th>
   <th scope="col">Comment</th>
  </tr>
  <tr>
   <td><a class="external" href="https://webaudio.github.io/web-audio-api/" hreflang="en" lang="en" title="Web Audio API">Web Audio API</a></td>
   <td><span class="spec-WD">Working Draft</span></td>
   <td></td>
  </tr>
 </tbody>
</table>

<h2 id="浏览器兼容性">浏览器兼容性</h2>

<div><div class="warning notecard"><strong><a href="https://github.com/mdn/browser-compat-data">We&apos;re converting our compatibility data into a machine-readable JSON format</a></strong>.
            This compatibility table still uses the old format,
            because we haven&apos;t yet converted the data it contains.
            <strong><a href="/zh-CN/docs/MDN/Contribute/Structures/Compatibility_tables">Find out how you can help!</a></strong></div>

<div class="htab">
    <a id="AutoCompatibilityTable" name="AutoCompatibilityTable"></a>
    <ul>
        <li class="selected"><a>Desktop</a></li>
        <li><a>Mobile</a></li>
    </ul>
</div></div>

<div id="compat-desktop">
<table class="compat-table">
 <tbody>
  <tr>
   <th>Feature</th>
   <th>Chrome</th>
   <th>Firefox (Gecko)</th>
   <th>Internet Explorer</th>
   <th>Opera</th>
   <th>Safari (WebKit)</th>
  </tr>
  <tr>
   <td>Basic support</td>
   <td>14 <span class="prefixBox prefixBoxInline notecard inline" title="prefix"><a href="/zh-CN/docs/Web/Guide/Prefixes" title="The name of this feature is prefixed with &apos;webkit&apos; as this browser considers it experimental">webkit</a></span></td>
   <td>23</td>
   <td><span style="color: #f00;">未实现</span></td>
   <td>15 <span class="prefixBox prefixBoxInline notecard inline" title="prefix"><a href="/zh-CN/docs/Web/Guide/Prefixes" title="The name of this feature is prefixed with &apos;webkit&apos; as this browser considers it experimental">webkit</a></span><br>
    22 (unprefixed)</td>
   <td>6 <span class="prefixBox prefixBoxInline notecard inline" title="prefix"><a href="/zh-CN/docs/Web/Guide/Prefixes" title="The name of this feature is prefixed with &apos;webkit&apos; as this browser considers it experimental">webkit</a></span></td>
  </tr>
 </tbody>
</table>
</div>

<div id="compat-mobile">
<table class="compat-table">
 <tbody>
  <tr>
   <th>Feature</th>
   <th>Android</th>
   <th>Chrome</th>
   <th>Firefox Mobile (Gecko)</th>
   <th>Firefox OS</th>
   <th>IE Phone</th>
   <th>Opera Mobile</th>
   <th>Safari Mobile</th>
  </tr>
  <tr>
   <td>Basic support</td>
   <td><span style="color: #f00;">未实现</span></td>
   <td>28 <span class="prefixBox prefixBoxInline notecard inline" title="prefix"><a href="/zh-CN/docs/Web/Guide/Prefixes" title="The name of this feature is prefixed with &apos;webkit&apos; as this browser considers it experimental">webkit</a></span></td>
   <td>25</td>
   <td>1.2</td>
   <td><span style="color: #f00;">未实现</span></td>
   <td><span style="color: #f00;">未实现</span></td>
   <td>6 <span class="prefixBox prefixBoxInline notecard inline" title="prefix"><a href="/zh-CN/docs/Web/Guide/Prefixes" title="The name of this feature is prefixed with &apos;webkit&apos; as this browser considers it experimental">webkit</a></span></td>
  </tr>
 </tbody>
</table>
</div>

<h2 id="相关链接">相关链接</h2>

<ul>
 <li><a href="/en-US/docs/Web/API/Web_Audio_API/Using_Web_Audio_API">Using the Web Audio API</a></li>
 <li><a href="/en-US/docs/Web/API/Web_Audio_API/Visualizations_with_Web_Audio_API">Visualizations with Web Audio API</a></li>
 <li><a href="http://mdn.github.io/voice-change-o-matic/">Voice-change-O-matic example</a></li>
 <li><a href="http://mdn.github.io/violent-theremin/">Violent Theremin example</a></li>
 <li><a href="/en-US/docs/Web/API/Web_Audio_API/Web_audio_spatialisation_basics">Web audio spatialisation basics</a></li>
 <li><a href="http://www.html5rocks.com/tutorials/webaudio/positional_audio/" title="http://www.html5rocks.com/tutorials/webaudio/positional_audio/">Mixing Positional Audio and WebGL</a></li>
 <li><a href="http://www.html5rocks.com/tutorials/webaudio/games/" title="http://www.html5rocks.com/tutorials/webaudio/games/">Developing Game Audio with the Web Audio API</a></li>
 <li><a href="/en-US/docs/Web/API/Web_Audio_API/Porting_webkitAudioContext_code_to_standards_based_AudioContext" title="/en-US/docs/Web_Audio_API/Porting_webkitAudioContext_code_to_standards_based_AudioContext">Porting webkitAudioContext code to standards based AudioContext</a></li>
 <li><a href="https://github.com/bit101/tones">Tones</a>: a simple library for playing specific tones/notes using the Web Audio API.</li>
 <li><a href="https://github.com/goldfire/howler.js/">howler.js</a>: a JS audio library that defaults to <a href="https://dvcs.w3.org/hg/audio/raw-file/tip/webaudio/specification.html">Web Audio API</a> and falls back to <a href="http://www.whatwg.org/specs/web-apps/current-work/#the-audio-element">HTML5 Audio</a>, as well as providing other useful features.</li>
</ul>

<section id="Quick_Links">
<h3 id="Quicklinks">Quicklinks</h3>

<ol>
 <li data-default-state="open"><strong><a href="#">Guides</a></strong>

  <ol>
   <li><a href="/zh-CN/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API">Basic concepts behind Web Audio API</a></li>
   <li><a href="/zh-CN/docs/Web/API/Web_Audio_API/Using_Web_Audio_API">Using the Web Audio API</a></li>
   <li><a href="/zh-CN/docs/Web/API/Web_Audio_API/Visualizations_with_Web_Audio_API">Visualizations with Web Audio API</a></li>
   <li><a href="/zh-CN/docs/Web/API/Web_Audio_API/Web_audio_spatialisation_basics">Web audio spatialisation basics</a></li>
   <li><a href="/zh-CN/docs/Web/API/Web_Audio_API/Porting_webkitAudioContext_code_to_standards_based_AudioContext">Porting webkitAudioContext code to standards based AudioContext</a></li>
  </ol>
 </li>
 <li data-default-state="open"><strong><a href="#">Examples</a></strong>
  <ol>
   <li><a href="http://mdn.github.io/voice-change-o-matic/">Voice-change-O-matic</a></li>
   <li><a href="http://mdn.github.io/violent-theremin/">Violent Theremin</a></li>
  </ol>
 </li>
 <li data-default-state="open"><strong><a href="#">Interfaces</a></strong>
  <ol>
   <li><a href="/zh-CN/docs/Web/API/AnalyserNode"><code>AnalyserNode</code></a></li>
   <li><a href="/zh-CN/docs/Web/API/AudioBuffer"><code>AudioBuffer</code></a></li>
   <li><a href="/zh-CN/docs/Web/API/AudioBufferSourceNode"><code>AudioBufferSourceNode</code></a></li>
   <li><a href="/zh-CN/docs/Web/API/AudioContext"><code>AudioContext</code></a></li>
   <li><a href="/zh-CN/docs/Web/API/AudioDestinationNode"><code>AudioDestinationNode</code></a></li>
   <li><a href="/zh-CN/docs/Web/API/AudioListener"><code>AudioListener</code></a></li>
   <li><a href="/zh-CN/docs/Web/API/AudioNode"><code>AudioNode</code></a></li>
   <li><a href="/zh-CN/docs/Web/API/AudioParam"><code>AudioParam</code></a></li>
   <li><code><a href="/zh-CN/docs/Web/Reference/Events/audioprocess">audioprocess</a></code> (event)</li>
   <li><a href="/zh-CN/docs/Web/API/AudioProcessingEvent"><code>AudioProcessingEvent</code></a></li>
   <li><a href="/zh-CN/docs/Web/API/BiquadFilterNode"><code>BiquadFilterNode</code></a></li>
   <li><a href="/zh-CN/docs/Web/API/ChannelMergerNode"><code>ChannelMergerNode</code></a></li>
   <li><a href="/zh-CN/docs/Web/API/ChannelSplitterNode"><code>ChannelSplitterNode</code></a></li>
   <li><code><a href="/zh-CN/docs/Web/Reference/Events/complete">complete</a></code> (event)</li>
   <li><a href="/zh-CN/docs/Web/API/ConvolverNode"><code>ConvolverNode</code></a></li>
   <li><a href="/zh-CN/docs/Web/API/DelayNode"><code>DelayNode</code></a></li>
   <li><a href="/zh-CN/docs/Web/API/DynamicsCompressorNode"><code>DynamicsCompressorNode</code></a></li>
   <li><code><a href="/zh-CN/docs/Web/Reference/Events/ended_(Web_Audio)">ended</a></code> (event)</li>
   <li><a href="/zh-CN/docs/Web/API/GainNode"><code>GainNode</code></a></li>
   <li><a href="/zh-CN/docs/Web/API/MediaElementAudioSourceNode"><code>MediaElementAudioSourceNode</code></a></li>
   <li><a href="/zh-CN/docs/Web/API/MediaStreamAudioDestinationNode"><code>MediaStreamAudioDestinationNode</code></a></li>
   <li><a href="/zh-CN/docs/Web/API/MediaStreamAudioSourceNode"><code>MediaStreamAudioSourceNode</code></a></li>
   <li><a href="/zh-CN/docs/Web/API/OfflineAudioCompletionEvent"><code>OfflineAudioCompletionEvent</code></a></li>
   <li><a href="/zh-CN/docs/Web/API/OfflineAudioContext"><code>OfflineAudioContext</code></a></li>
   <li><a href="/zh-CN/docs/Web/API/OscillatorNode"><code>OscillatorNode</code></a></li>
   <li><a href="/zh-CN/docs/Web/API/PannerNode"><code>PannerNode</code></a></li>
   <li><a href="/zh-CN/docs/Web/API/PeriodicWaveNode"><code>PeriodicWaveNode</code></a></li>
   <li><a href="/zh-CN/docs/Web/API/ScriptProcessorNode"><code>ScriptProcessorNode</code></a></li>
   <li><a href="/zh-CN/docs/Web/API/WaveShaperNode"><code>WaveShaperNode</code></a></li>
  </ol>
 </li>
</ol>
</section>
