---
title: Utiliser la Web Audio API
slug: Web/API/Web_Audio_API/Using_Web_Audio_API
translation_of: Web/API/Web_Audio_API/Using_Web_Audio_API
---
<div class="summary">
<p><span class="seoSummary">La <a href="/en-US/docs/Web_Audio_API">Web Audio API</a> offre un méchanisme à la fois simple et puissant pour implémenter et manipuler le contenu audio dans une application web. Elle permet de manipuler mixages audio, effets, balance, etc. Cet article donne les bases pour l&apos;utiliser, à travers quelques exemples simples..</span></p>
</div>

<div>
<p>La Web Audio API ne vient pas remplacer l&apos;élément <a href="/en-US/docs/Web/HTML/Element/audio">&lt;audio&gt;</a>, mais plutôt le compléter, de même que l&apos;API Canvas 2D coexiste avec l&apos;élément <a href="/en-US/docs/Web/HTML/Element/Img">&lt;video&gt;</a>. Si vous avez seulement besoin de contrôler la lecture d&apos;un fichier audio, &lt;audio&gt; est probablement une meilleure solution, plus rapide. Si vous voulez procéder à un traitement audio plus complexe et à la lecture d&apos;une source, la Web Audio API offre davantage de possibilités en termes de puissance et de contrôle.</p>

<p>L&apos;une des particularités de la Web Audio API est qu&apos;elle n&apos;a pas de limites au niveau de la programmation du son. Par exemple, le nombre de sons que l&apos;on peut appeler en même temps n&apos;est pas plafonnée. Certains processeurs sont potentiellement capables de jouer plus d&apos;un millier de sons simultanément sans saccades.</p>
</div>

<h2 id="Exemples">Exemples</h2>

<p>Afin d&apos;expliquer l&apos;utilisation de la Web Audio API, nous avons créé un certain nombre d&apos;exemples qui seront étoffés au fur et à mesure. N&apos;hésitez pas à en ajouter d&apos;autres et à suggérer des améliorations !</p>

<p>Notre premier exemple est <a href="http://github.com/mdn/voice-change-o-matic">Voice-change-O-matic</a>, une application web de déformation de la voix, qui permet de choisir différents effets et modes de visualisation. Cette application est rudimentaire, mais elle permet de montrer l&apos;utilisation de plusieurs fonctionnalités de la Web Audio API combinées ensemble  (<a href="http://mdn.github.io/voice-change-o-matic/">run the Voice-change-O-matic live</a>).</p>

<p><img src="https://mdn.mozillademos.org/files/7921/voice-change-o-matic.png" alt="A UI with a sound wave being shown, and options for choosing voice effects and visualizations." style="display: block; height: 500px; margin: 0px auto; width: 640px;"></p>

<p>Un autre exemple pour illustrer la Web Audio API est le <a href="http://mdn.github.io/violent-theremin/">Violent Theremin</a>, une application simple qui permet de changer le pitch et le volume d&apos;un son en fonction du déplacement de la souris. Elle génère également des animations psychédéliques (<a href="https://github.com/mdn/violent-theremin">see Violent Theremin source code</a>).</p>

<p><img src="https://mdn.mozillademos.org/files/7919/violent-theremin.png" alt="A page full of rainbow colours, with two buttons labeled Clear screen and mute. " style="display: block; height: 458px; margin: 0px auto; width: 640px;"></p>

<h2 id="Concepts_de_base">Concepts de base</h2>

<div class="note notecard">
<p><strong>Note</strong>: la plupart des extraits de code dans cette section viennent de l&apos;exemple <a href="https://github.com/mdn/violent-theremin">Violent Theremin</a>.</p>
</div>

<p>La Web Audio API impliqe de réaliser les opérations de traitement audio dans un <strong>contexte audio</strong>, et elle a été conçue pour permettre le <strong>routage modulaire</strong>. Les opérations de traitement de base sont réalisées par des <strong> noeuds audio</strong>, qui sont reliés entre eux pour former un<strong> graphe de routage audio</strong>. Plusieurs sources — avec différentes configuration de canaux — peuvent cohabiter dans un seul contexte. Ce design modulaire offre la flexibilité nécessaire pour créer des fonctions complexes avec des effets dynamiques.</p>

<p>Les noeuds audio sont reliés au niveau de leurs entrées et sorties. Ils forment une chaîne qui commence avec une ou plusieurs sources, traverse un ou plusieurs noeuds de traitement, et se termine par une destination (bien qu&apos;il ne soit pas néessaire d&apos;avoir une destination si l&apos;on souhaite simplement visualiser des données audio). Un scénario simple, représentatif de la Web Audio API, pourrait ressembler à ceci :</p>

<ol>
 <li>Création d&apos;un contexte audio</li>
 <li>Dans ce contexte, création des sources — telles que <code>&lt;audio&gt;</code>, oscillateur, flux</li>
 <li>Création des noeuds d&apos;effets, tels que réverb, filtres biquad, balance,  compresseur</li>
 <li>Choix final de la sortie audio, par exemple les enceintes du système </li>
 <li>Connection des sources aux effets, et des effets à la sortie.</li>
</ol>

<h3 id="Création_d&apos;un_contexte_audio">Création d&apos;un contexte audio</h3>

<p>Commencez par créer une instance de <a href="/en-US/docs/Web/API/AudioContext"><code>AudioContext</code></a> sur laquelle vous allez créer un graphe audio. L&apos;exemple le plus simple ressemblerait à ceci:</p>

<pre class="brush: js">var contexteAudio = new AudioContext();
</pre>

<div class="note notecard">
<p><strong>Note</strong>: On peut créer plusieurs contextes audio dans le même document, bien que ce soit probablement superflu dans la plupart des cas.</p>
</div>

<p>Il faut rajouter une version préfixée pour les navigateurs Webkit/Blink browsers, tout en conservant la version non préfixée pour Firefox (desktop/mobile/OS). Ce qui donne :</p>

<pre class="brush: js">var contexteAudio = new (window.AudioContext || window.webkitAudioContext)();
</pre>

<div class="note notecard">
<p><strong>Note</strong>: <span style="line-height: 1.5;">Safari risque de planter si l&apos;objet </span><code style="line-height: 1.5; font-style: normal;">window</code><span style="line-height: 1.5;"> n&apos;est pas explicitement mentionné lors de la création d&apos;un contexte audio</span></p>
</div>

<h3 id="Création_d&apos;une_source_audio">Création d&apos;une source audio</h3>

<p>Maintenant que nous avons créé un contexte, nous allons utiliser les méthodes de ce contexte pour quasiment tout ce qui nous reste à faire. La première étape consiste à créer une source audio. Les sources peuvent être de provenance diverse :</p>

<ul>
 <li>générées en JavaScript par un noeud audio tel qu&apos;un oscillateur. Pour créer un <a href="/fr/docs/Web/API/OscillatorNode"><code>OscillatorNode</code></a> on utilise la méthode <a href="/fr/docs/Web/API/AudioContext/createOscillator"><code>AudioContext.createOscillator</code></a>.</li>
 <li>créées à partir de données PCM brutes: le contexte audio a des méthodes pour décoder lesformats supportés; voir <a href="/fr/docs/Web/API/AudioContext/createBuffer"><code>AudioContext.createBuffer()</code></a>, <a href="/fr/docs/Web/API/AudioContext/createBufferSource"><code>AudioContext.createBufferSource()</code></a>, et <a href="/fr/docs/Web/API/AudioContext/decodeAudioData"><code>AudioContext.decodeAudioData()</code></a>.</li>
 <li>récupérées dans des élements HTML tels que <a href="/fr/docs/Web/HTML/Element/video"><code>&lt;video&gt;</code></a> ou <a href="/fr/docs/Web/HTML/Element/audio"><code>&lt;audio&gt;</code></a>: voir <a href="/fr/docs/Web/API/AudioContext/createMediaElementSource"><code>AudioContext.createMediaElementSource()</code></a>.</li>
 <li>prises dans un <a href="/en-US/docs/WebRTC">WebRTC</a> <a href="/fr/docs/Web/API/MediaStream"><code>MediaStream</code></a> comme une webcam ou un microphone. Voir <a href="/fr/docs/Web/API/AudioContext/createMediaStreamSource"><code>AudioContext.createMediaStreamSource()</code></a>.</li>
</ul>

<p>Pour notre exemple nous nous contenterons de créer un oscillateur pour générer un son simple comme source, et un noeud de gain pour contrôler le volume:</p>

<pre class="brush: js">var oscillateur = contexteAudio.createOscillator();
var noeudGain = contexteAudio.createGain();
</pre>

<div class="note notecard">
<p><strong>Note</strong>: Pour jouer un fichier audio directement, il faut charger le fichier en XHR, le décoder en mémoire tampon, puis associer le tampon à une source. Voir l&apos;exemple <a href="https://github.com/mdn/voice-change-o-matic/blob/gh-pages/scripts/app.js#L48-L68">Voice-change-O-matic</a>.</p>
</div>

<div class="note notecard">
<p><strong>Note</strong>: Scott Michaud a écrit la librairie <a href="https://github.com/ScottMichaud/AudioSampleLoader">AudioSampleLoader</a>, très pratique pour charger et décoder un ou plusieurs extraits audio. Elle peut aider à simplifier le processus de chargement XHR / mémoire tampon décrit dans la note précédente.</p>
</div>

<h3 id="Lien_entre_les_noeuds_source_et_destination">Lien entre les noeuds source et destination</h3>

<p>Pour faire sortir le son dans vos enceintes, il faut relier la source et la destination. Pour cela on appelle la méthode <code>connect</code> sur le noeud source, le noeud de destination étant passé en argument. La méthode <code>connect</code> est disponible sur la plupart des types de noeuds.<code> </code></p>

<p>La sortie par défaut du matériel (en général les enceintes) est accessible via <a href="/fr/docs/Web/API/AudioContext/destination" title="Cette documentation n&apos;a pas encore été rédigée, vous pouvez aider en contribuant !"><code>AudioContext.destination</code></a>. Pour connecter l&apos;oscillateur, le noeud de gain et la destination, on écrirait les lignes suivantes:</p>

<pre class="brush: js">oscillateur.connect(noeudGain);
noeudGain.connect(contexteAudio.destination);
</pre>

<p>On peut connecter autant de noeuds qu&apos;on le souhaite (cf. <a href="http://mdn.github.io/voice-change-o-matic/">Voice-change-O-matic</a>). Par exemple:</p>

<pre class="brush: js">source = contexteAudio.createMediaStreamSource(stream);
source.connect(analyser);
analyser.connect(distortion);
distortion.connect(biquadFilter);
biquadFilter.connect(convolver);
convolver.connect(noeudGain);
noeudGain.connect(contexteAudio.destination);
</pre>

<p>Ce code créerait le graphe audio suivant :</p>

<p><img src="https://mdn.mozillademos.org/files/7949/voice-change-o-matic-graph.png" alt style="display: block; height: 563px; margin: 0px auto; width: 232px;">Il est possible de connecter plusieurs noeuds à un seul noeud, par exemple pour mixer plusieurs sources ensemble, et les passer dans un seul noeud d&apos;effet, tel qu&apos;un noeud de gain.</p>

<div class="note notecard">
<p><strong>Note</strong>: Depuis Firefox 32, les outils de développement intégrés incluent un <a href="/en-US/docs/Tools/Web_Audio_Editor">éditeur audio</a>,  très utile pour débugger les graphes audio.</p>
</div>

<h3 id="Lecture_du_son_et_définition_du_pitch">Lecture du son et définition du pitch</h3>

<p>Maintenant que le graphe audio est en place, nous pouvons ajuster certains aspects du son en définissant la valeur de certaines propriétés ou en utilsant ses méthodes. L&apos;exemple suivant spécifie un pitch en hertz pour un oscillateur, lui assigne un type, et demande à l&apos;oscillateur de jouer le son.</p>

<pre class="brush: js">oscillateur.type = &apos;sine&apos;; // onde sinusoïdale — les autres valeurs possible sont : &apos;square&apos;, &apos;sawtooth&apos;, &apos;triangle&apos; et &apos;custom&apos;
oscillateur.frequency.value = 2500; // valeur en hertz
oscillateur.start();
</pre>

<p>Le code suivant, qui vient de l&apos;exemple <a href="http://mdn.github.io/violent-theremin/">Violent Theremin</a>, spécifie une valeur maximum pour le gain, et une valeur pour la fréquence:</p>

<pre class="brush: js">var largeur = window.innerWidth;
var hauteur = window.innerHeight;

var frequenceMax = 6000;
var volumeMax = 1;

var frequenceInitiale = 3000;
var volumeInitial = 0.5;

// paramètres de l&apos;oscillateur

oscillateur.type = &apos;sine&apos;; // onde sinusoïdale — les autres valeurs possible sont : &apos;square&apos;, &apos;sawtooth&apos;, &apos;triangle&apos; et &apos;custom&apos;
oscillateur.frequency.value = frequenceInitiale; // valeur en hertz
oscillateur.start();

noeudGain.gain.value = volumeInitial;
</pre>

<p>On peut aussi réassigner les valeurs de fréquence et de pitch à chaque mouvement de la souris, en utilisant la position pour calculer un pourcentage des valeurs maximum de fréquence et de gain :</p>

<pre class="brush: js">// coordonnées de la souris

var positionX;
var positionY;

// récupère les nouvelles coordonnées de la souris quand elle bouge
// puis assigne les nouvelles valeurs de gain et de pitch

document.onmousemove = updatePage;

function updatePage(e) {
    positionX = (window.Event) ? e.pageX : e.clientX + (document.documentElement.scrollLeft ? document.documentElement.scrollLeft : document.body.scrollLeft);
    positionY = (window.Event) ? e.pageY : e.clientY + (document.documentElement.scrollTop ? document.documentElement.scrollTop : document.body.scrollTop);

    oscillateur.frequency.value = (positionX/largeur) * frequenceMax;
    noeudGain.gain.value = (positionY/hauteur) * volumeMax;

    canvasDraw();
}
</pre>

<h3 id="Simple_visualisation_avec_canvas">Simple visualisation avec canvas</h3>

<p>On appelle une fonction <code>canvasDraw()</code> à chaque mouvement de la souris. Elle dessine une grappe de cercles à l&apos;endroit où se trouve la souris, leur taille et couleur étant basées sur les valeurs de fréquence et de gain.</p>

<pre class="brush: js">function aleatoire(number1, number2) {
  return number1 + (Math.floor(Math.random() * (number2 - number1)) + 1);
}

var canvas = document.querySelector(&apos;.canvas&apos;);
canvas.width = largeur;
canvas.height = hauteur;

var contexteCanvas = canvas.getContext(&apos;2d&apos;);

function canvasDraw() {
  rX = positionX;
  rY = positionY;
  rC = Math.floor((noeudGain.gain.value / volumeMax) * 30);

  canvasCtx.globalAlpha = 0.2;

  for(i=1;i&lt;=15;i=i+2) {
    contexteCanvas.beginPath();
    var chaineStyle = &apos;rgb(&apos; + 100 + (i * 10) + &apos;,&apos; + Math.floor((noeudGain.gain.value / volumeMax) * 255);
    chaineStyle += &apos;,&apos; + Math.floor((oscillateur.frequency.value / frequenceMax) * 255) + &apos;)&apos;;
    contexteCanvas.fillStyle = chaineStyle;
    contexteCanvas.arc(rX + aleatoire(0, 50), rY + aleatoire(0, 50), rC / 2 + i, (Math.PI / 180) * 0, (Math.PI / 180) * 360, false);
    contexteCanvas.fill();
    contexteCanvas.closePath();
  }
}</pre>

<h3 id="Couper_le_son_du_theremin">Couper le son du theremin</h3>

<p>Quand on appuie sur le bouton pour couper le son, la fonction ci-dessous est appelée, qui déconnecte le noeud de gain du noeud de destination, cassant ainsi le graphe de façon à ce qu&apos;aucun son ne soit produit. Appuyer de nouveau sur le bouton a l&apos;effet inverse.</p>

<pre class="brush: js">var coupeSon = document.querySelector(&apos;.mute&apos;);

coupeSon.onclick = function() {
  if(coupeSon.id == &quot;&quot;) {
    noeudGain.disconnect(contexteAudio.destination);
    coupeSon.id = &quot;activated&quot;;
    coupeSon.innerHTML = &quot;Unmute&quot;;
  } else {
    noeudGain.connect(contexteAudio.destination);
    coupeSon.id = &quot;&quot;;
    coupeSon.innerHTML = &quot;Mute&quot;;
  }
}
</pre>

<h2 id="Autres_options_des_noeuds">Autres options des noeuds</h2>

<p>On peut créer un grand nombre d&apos;autres noeuds avec la Web Audio API. De façon générale, ils fonctionnent de façon très similaire à ceux que nous venons de voir: on crée un noeud, le connecte avec d&apos;autres noeuds, et on manipule ensuite ses propriétés et méthodes pour agir sur la source.</p>

<p>Ce document passe en revue quelques-uns des outils et effets disponibles; vous trouverez davantage de détails sur les pages de référence de la <a href="/fr/docs/Web/API/Web_Audio_API" title="La Web Audio API effectue des opérations dans un contexte audio; elle a été conçue pour supporter le routing modulaire. Les opérations audio basiques sont réalisées via des noeuds audio reliés entre eux pour former un graphe de routage audio. Plusieurs sources - avec différents types d&apos;agencements de canaux - peuvent être supportées, même dans un seul contexte. Ce design modulaire et flexible permet de créer des fonctions audio complexes avec des effets dynamiques."><code>Web_Audio_API</code></a>.</p>

<h3 id="Noeuds_modulateurs_d&apos;onde">Noeuds modulateurs d&apos;onde</h3>

<p>On peut créer un noeud modulatur d&apos;onde avec la méthode <a href="/fr/docs/Web/API/AudioContext/createWaveShaper" title="Cette documentation n&apos;a pas encore été rédigée, vous pouvez aider en contribuant !"><code>AudioContext.createWaveShaper</code></a> :</p>

<pre class="brush: js">var distortion = contexteAudio.createWaveShaper();
</pre>

<p>On associe ensuite à cet objet une forme d&apos;onde définie mathématiquement, qui est appliquée à l&apos;onde de base pour créer un effet de distortion. Ecrire son propre algorithme n&apos;est pas si simple, et pour commencer le mieux est encore d&apos;en chercher un sur le Web. Par exemple, nous avons trouvé celui-ci sur <a href="http://stackoverflow.com/questions/22312841/waveshaper-node-in-webaudio-how-to-emulate-distortion">Stack Overflow</a>:</p>

<pre class="brush: js">function genererCourbeDistortion(amount) {
  var k = typeof amount === &apos;number&apos; ? amount : 50,
    n_samples = 44100,
    curve = new Float32Array(n_samples),
    deg = Math.PI / 180,
    i = 0,
    x;
  for ( ; i &lt; n_samples; ++i ) {
    x = i * 2 / n_samples - 1;
    curve[i] = ( 3 + k ) * x * 20 * deg / ( Math.PI + k * Math.abs(x) );
  }
  return curve;
};
</pre>

<p>L&apos;exemple suivant, qui vient de <a href="https://github.com/mdn/voice-change-o-matic">Voice-change-O-matic</a>, connecte un noeud de  <code>distortion</code> à un graphe audio, puis applique l&apos;algorithme de forme d&apos;onde précédent au noeud de distortion :</p>

<pre class="brush: js">source.connect(analyser);
analyser.connect(distortion);
distortion.connect(biquadFilter);

...

distortion.curve = genererCourbeDistortion(400);
</pre>

<h3 id="Filtre_biquad">Filtre biquad</h3>

<p>Les filtres biquad ont de nombreuses options. Nous montrons ici comment créer un filtre biquad avec la méthode <a href="/fr/docs/Web/API/AudioContext/createBiquadFilter" title="Cette documentation n&apos;a pas encore été rédigée, vous pouvez aider en contribuant !"><code>AudioContext.createBiquadFilter</code></a>.</p>

<pre class="brush: js">var filtreBiquad = contexteAudio.createBiquadFilter();
</pre>

<p>Le filtre utilisé dans la démo Voice-change-o-matic est un filtre lowshelf, qui amplifie le son au niveau des basses. Ici on augmente de 25 décibels toutes les fréquences en dessous de 1000.</p>

<pre class="brush: js">filtreBiquad.type = &quot;lowshelf&quot;;
filtreBiquad.frequency.value = 1000;
filtreBiquad.gain.value = 25;
</pre>

<h2 id="Autres_usages_de_la_Web_Audio_API">Autres usages de la Web Audio API</h2>

<p>La Web Audio API peut avoir bien d&apos;autres applications que la visualisation ou la spatialisation audio (comme gérer la balance d&apos;un son). Nous détaillerons d&apos;autres options dans des articles séparés.</p>
