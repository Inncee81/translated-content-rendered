---
title: Используем Web Audio API
slug: Web/API/Web_Audio_API/Using_Web_Audio_API
translation_of: Web/API/Web_Audio_API/Using_Web_Audio_API
---
<div class="summary">
<p><span class="seoSummary"><a href="/en-US/docs/Web_Audio_API">Web Audio API</a> обеспечивает простой, но мощный механизм реализации и управления аудио-контентом внутри web приложения. Это позволяет разрабатывать сложные аудио миксины, эффекты и т.д. В этой статье мы постараемся обьяснить основы использования Web Audio API, посредством пары простых примеров.</span></p>
</div>

<div>
<p>The Web Audio API does not replace the <a href="/en-US/docs/Web/HTML/Element/audio">&lt;audio&gt;</a> media element, but rather complements it, just like <a href="/en-US/docs/Web/HTML/Element/canvas">&lt;canvas&gt;</a> co-exists well alongside the <a href="/en-US/docs/Web/HTML/Element/Img">&lt;img&gt;</a> element. What you use to implement audio depends on your use case. If you just want to control playback of a simple audio track, &lt;audio&gt; is probably a better, quicker solution. If you want to carry out more complex audio processing, as well as playback, Web Audio API provides much more power and control.</p>

<p>One very powerful thing about the Web Audio API is that it does not have any strict &quot;sound call limitations&quot;. There is no ceiling of 32 or 64 sound calls at one time, for example. Depending on the power of your processor, you might be able to get a thousand or more sounds playing simultaneously without stuttering. This shows real progress, given that a few years ago mid to high range sound cards were able to handle only a fraction of this load.</p>
</div>

<h2 id="Examples">Examples</h2>

<p>To demonstrate usage of the Web Audio API, we created a number of examples which will be added to as time goes on. Please feel free to add to them and suggest improvements!</p>

<p>First off, we created the <a href="https://github.com/mdn/voice-change-o-matic">Voice-change-O-matic</a>, a fun voice changer and sound visualisation web app, which allows you to choose different effects and visualisations. This could definitely be improved on with better quality effects, but it demonstrates the usage of multiple Web Audio API features together (<a href="http://mdn.github.io/voice-change-o-matic/">run the Voice-change-O-matic live</a>).</p>

<p><img src="https://mdn.mozillademos.org/files/7921/voice-change-o-matic.png" alt="A UI with a sound wave being shown, and options for choosing voice effects and visualizations." style="display: block; height: 500px; margin: 0px auto; width: 640px;"></p>

<p>Another example we&apos;ve created on our quest to understand the Web Audio is the <a href="http://mdn.github.io/violent-theremin/">Violent Theremin</a>, a simple app allowing you to change the pitch and volume by moving your mouse pointer. It also provides a psychedelic lightshow (<a href="https://github.com/mdn/violent-theremin">see Violent Theremin source code</a>).</p>

<p><img src="https://mdn.mozillademos.org/files/7919/violent-theremin.png" alt="A page full of rainbow colours, with two buttons labeled Clear screen and mute. " style="display: block; height: 458px; margin: 0px auto; width: 640px;"></p>

<h2 id="Basic_concepts">Basic concepts</h2>

<div class="note notecard">
<p><strong>Note</strong>: most of the code snippets in this example are taken from the <a href="https://github.com/mdn/violent-theremin">Violent Theremin example</a>.</p>
</div>

<p>The Web Audio API involves handling audio operations inside an <strong>audio context</strong>, and has been designed to allow <strong>modular routing</strong>. Basic audio operations are performed with <strong>audio nodes</strong>, which are linked together to form an <strong>audio routing graph</strong>. Several sources — with different types of channel layouts — are supported even within a single context. This modular design provides the flexibility to create complex audio functions with dynamic effects.</p>

<p>Audio nodes are linked via their inputs and outputs, forming a chain that starts with one or more sources, goes through one or more nodes, then ends up at a destination (although you don&apos;t have to provide a destination if you, say, just want to visualise some audio data). A simple, typical workflow for web audio would look something like this:</p>

<ol>
 <li>Create audio context</li>
 <li>Inside the context, create sources — such as <code>&lt;audio&gt;</code>, oscillator, stream</li>
 <li>Create effects nodes, such as reverb, biquad filter, panner, compressor</li>
 <li>Choose final destination of audio, for example your system speakers</li>
 <li>Connect the sources up to the effects, and the effects to the destination</li>
</ol>

<h3 id="Creating_an_audio_context">Creating an audio context</h3>

<p>First, you will need to create an instance of <a href="/en-US/docs/Web/API/AudioContext"><code>AudioContext</code></a> to build an audio graph upon. The simplest example of this would look like so:</p>

<pre class="brush: js">var audioCtx = new AudioContext();
</pre>

<div class="note notecard">
<p><strong>Note</strong>: Multiple audio context instances are allowed on the same document, but are likely wasteful.</p>
</div>

<p>However, it is important to provide a prefixed version for Webkit/Blink browsers, and a non-prefixed version for Firefox (desktop/mobile/OS). Something like this would work:</p>

<pre class="brush: js">var audioCtx = new (window.AudioContext || window.webkitAudioContext)();
</pre>

<div class="note notecard">
<p><strong>Note</strong>: <span style="line-height: 1.5;">Safari can break if you don&apos;t explcitly mention the </span><code style="line-height: 1.5; font-style: normal;">window</code><span style="line-height: 1.5;"> object when creating a new context!</span></p>
</div>

<h3 id="Creating_an_audio_source">Creating an audio source</h3>

<p>Now we have an audio context, and we can use the methods of this context to do most everything else. The first thing we need is an audio source to play around with. Audio sources can come from a variety of places:</p>

<ul>
 <li>Generated directly by JavaScript by an audio node such as an oscillator. An <a href="/ru/docs/Web/API/OscillatorNode"><code>OscillatorNode</code></a> is created using the <a href="/ru/docs/Web/API/AudioContext/createOscillator"><code>AudioContext.createOscillator</code></a> method.</li>
 <li>Created from raw PCM data: the audio context has methods to decode supported audio formats; see <a href="/ru/docs/Web/API/AudioContext/createBuffer"><code>AudioContext.createBuffer()</code></a>, <a href="/ru/docs/Web/API/AudioContext/createBufferSource"><code>AudioContext.createBufferSource()</code></a>, and <a href="/ru/docs/Web/API/AudioContext/decodeAudioData"><code>AudioContext.decodeAudioData()</code></a>.</li>
 <li>Taken from HTML media elements such as <a href="/ru/docs/Web/HTML/Element/video"><code>&lt;video&gt;</code></a> or <a href="/ru/docs/Web/HTML/Element/audio"><code>&lt;audio&gt;</code></a>: see <a href="/ru/docs/Web/API/AudioContext/createMediaElementSource"><code>AudioContext.createMediaElementSource()</code></a>.</li>
 <li>Taken directly from a <a href="/en-US/docs/WebRTC">WebRTC</a> <a href="/ru/docs/Web/API/MediaStream"><code>MediaStream</code></a> such as from a webcam or microphone. See <a href="/ru/docs/Web/API/AudioContext/createMediaStreamSource"><code>AudioContext.createMediaStreamSource()</code></a>.</li>
</ul>

<p>For this particular example we&apos;ll just create an oscillator to provide a simple tone for our source, and a gain node to control sound volume:</p>

<pre class="brush: js">oscillator = audioCtx.createOscillator();
var gainNode = audioCtx.createGain();
</pre>

<div class="note notecard">
<p><strong>Note</strong>: To play a music file directly, you generally have to load the file using XHR, decode the file into a buffer, and then feed that buffer into a buffer source. See this <a href="https://github.com/mdn/voice-change-o-matic/blob/gh-pages/scripts/app.js#L48-L68">example from Voice-change-O-matic</a>.</p>
</div>

<div class="note notecard">
<p><strong>Note</strong>: Scott Michaud has written a useful helper library for loading and decoding one or more audio samples, called <a href="https://github.com/ScottMichaud/AudioSampleLoader">AudioSampleLoader</a>. This can help simplify the XHR/buffering process described in the above note.</p>
</div>

<h3 id="Linking_source_and_destination_together">Linking source and destination together</h3>

<p>To actually output the tone through your speakers, you need to link them together. This is done by calling the <code>connect</code> method on the node you want to connect from, which is available on most node types. The node that you want to connect to is given as the argument of the <code>connect</code> method.</p>

<p>The default output mechanism of your device (usually your device speakers) is accessed using <a href="/ru/docs/Web/API/AudioContext/destination" title="Документация об этом ещё не написана; пожалуйста, поспособствуйте её написанию!"><code>AudioContext.destination</code></a>. To connect the oscillator, gain node and destination together, we would use the following:</p>

<pre class="brush: js">oscillator.connect(gainNode);
gainNode.connect(audioCtx.destination);
</pre>

<p>In a more complex example (such as the <a href="http://mdn.github.io/voice-change-o-matic/">Voice-change-O-matic</a>), you can chain together as many nodes as you want. For example:</p>

<pre class="brush: js">source = audioCtx.createMediaStreamSource(stream);
source.connect(analyser);
analyser.connect(distortion);
distortion.connect(biquadFilter);
biquadFilter.connect(convolver);
convolver.connect(gainNode);
gainNode.connect(audioCtx.destination);
</pre>

<p>This would create an audio graph like this:</p>

<p><img src="https://mdn.mozillademos.org/files/7949/voice-change-o-matic-graph.png" alt style="display: block; height: 563px; margin: 0px auto; width: 232px;">You can also link multiple nodes to one node, for example if you wanted to mix multiple audio sources together, passing them all through a single effect node, like a gain node.</p>

<div class="note notecard">
<p><strong>Note</strong>: From Firefox 32 onwards, the integrated Firefox Developer Tools include a <a href="/en-US/docs/Tools/Web_Audio_Editor">Web Audio Editor</a>, which is very useful for debugging web audio graphs.</p>
</div>

<h3 id="Playing_our_sound_and_setting_a_pitch">Playing our sound and setting a pitch</h3>

<p>Now that the audio graph is set up, we can set property values and invoke methods on our audio nodes to adjust the effect they have on the sound. In this simple example we can set a specific pitch in hertz for our oscillator, set it to a specific type, and tell our sound to play like so:</p>

<pre class="brush: js">oscillator.type = &apos;sine&apos;; // sine wave — other values are &apos;square&apos;, &apos;sawtooth&apos;, &apos;triangle&apos; and &apos;custom&apos;
oscillator.frequency.value = 2500; // value in hertz
oscillator.start();
</pre>

<p>In our Violent Theremin example we actually specify a max gain and frequency value:</p>

<pre class="brush: js">var WIDTH = window.innerWidth;
var HEIGHT = window.innerHeight;

var maxFreq = 6000;
var maxVol = 1;

var initialFreq = 3000;
var initialVol = 0.5;

// set options for the oscillator

oscillator.type = &apos;sine&apos;; // sine wave — other values are &apos;square&apos;, &apos;sawtooth&apos;, &apos;triangle&apos; and &apos;custom&apos;
oscillator.frequency.value = initialFreq; // value in hertz
oscillator.start();

gainNode.gain.value = initialVol;
</pre>

<p>Then we set a new value of frequency and pitch every time the mouse cursor is moved, based on the current mouse X and Y value as a percetage of the maximum frequency and gain:</p>

<pre class="brush: js">// Mouse pointer coordinates

var CurX;
var CurY;

// Get new mouse pointer coordinates when mouse is moved
// then set new gain and putch values

document.onmousemove = updatePage;

function updatePage(e) {
    CurX = (window.Event) ? e.pageX : event.clientX + (document.documentElement.scrollLeft ? document.documentElement.scrollLeft : document.body.scrollLeft);
    CurY = (window.Event) ? e.pageY : event.clientY + (document.documentElement.scrollTop ? document.documentElement.scrollTop : document.body.scrollTop);

    oscillator.frequency.value = (CurX/WIDTH) * maxFreq;
    gainNode.gain.value = (CurY/HEIGHT) * maxVol;

    canvasDraw();
}
</pre>

<h3 id="A_simple_canvas_visualization">A simple canvas visualization</h3>

<p>A <code>canvasDraw()</code> function is also invoked after each mouse movement, this draws a little cluster of circles where the mouse pointer currently is, the size and colour of which are based on the frequency/gain values.</p>

<pre class="brush: js">function random(number1,number2) {
  var randomNo = number1 + (Math.floor(Math.random() * (number2 - number1)) + 1);
  return randomNo;
}

var canvas = document.querySelector(&apos;.canvas&apos;);
canvas.width = WIDTH;
canvas.height = HEIGHT;

var canvasCtx = canvas.getContext(&apos;2d&apos;);

function canvasDraw() {
  rX = CurX;
  rY = CurY;
  rC = Math.floor((gainNode.gain.value/maxVol)*30);

  canvasCtx.globalAlpha = 0.2;

  for(i=1;i&lt;=15;i=i+2) {
    canvasCtx.beginPath();
    canvasCtx.fillStyle = &apos;rgb(&apos; + 100+(i*10) + &apos;,&apos; + Math.floor((gainNode.gain.value/maxVol)*255) + &apos;,&apos; + Math.floor((oscillator.frequency.value/maxFreq)*255) + &apos;)&apos;;
    canvasCtx.arc(rX+random(0,50),rY+random(0,50),rC/2+i,(Math.PI/180)*0,(Math.PI/180)*360,false);
    canvasCtx.fill();
    canvasCtx.closePath();
  }
}</pre>

<h3 id="Muting_the_theremin">Muting the theremin</h3>

<p>When the Mute button is pressed, the function seen below is invoked, which disconnects the gain node from the destination node, effectively breaking the graph up so no sound can be produced. Pressing it again reverses the effect.</p>

<pre class="brush: js">var mute = document.querySelector(&apos;.mute&apos;);

mute.onclick = function() {
  if(mute.id == &quot;&quot;) {
    gainNode.disconnect(audioCtx.destination);
    mute.id = &quot;activated&quot;;
    mute.innerHTML = &quot;Unmute&quot;;
  } else {
    gainNode.connect(audioCtx.destination);
    mute.id = &quot;&quot;;
    mute.innerHTML = &quot;Mute&quot;;
  }
}
</pre>

<h2 id="Other_node_options">Other node options</h2>

<p>There are many other nodes you can create using the Web Audio API, and the good news is that, in general, they work in the same way as what we have already seen: you create a node, connect it to the other nodes in the graph, and then manipulate that node&apos;s properties and methods to affect the sound source in the way you want.</p>

<p>We won&apos;t go through all of the available effects and so on; you can find details of how to use each one in the different interface reference pages of the <a href="/ru/docs/Web/API/Web_Audio_API" title="Web audio API позволяет обрабатывать операции над аудио с помощью специального аудио контекста (audio context), и был спроектирован на использование модульной маршрутизации (modular routing). Базовые операции выполняются с помощью аудио узлов (audio nodes), что объединяются вместе, формируя аудио-маршрутизаторную *таблицу (audio routing graph). Несколько источников - с разными видами поточных схем - поддерживаются даже изнутри простого контекста. Эта модульная концепция обеспечивает гибкость в создании сложных функций для динамических эффектов."><code>Web_Audio_API</code></a> reference. For now we&apos;ll just go through a couple of options.</p>

<h3 id="Wave_shaper_nodes">Wave shaper nodes</h3>

<p>You can create a wave shaper node using the <a href="/ru/docs/Web/API/AudioContext/createWaveShaper" title="Документация об этом ещё не написана; пожалуйста, поспособствуйте её написанию!"><code>AudioContext.createWaveShaper</code></a> method:</p>

<pre class="brush: js">var distortion = audioCtx.createWaveShaper();
</pre>

<p>This object must then be fed a mathmatically defined wave shape, which is applied to the base sound wave to create a distortion effect. These waves are not that easy to figure out, and the best way to start is to just search the Web for algorithms. For example, we found this on <a href="http://stackoverflow.com/questions/22312841/waveshaper-node-in-webaudio-how-to-emulate-distortion">Stack Overflow</a>:</p>

<pre class="brush: js">function makeDistortionCurve(amount) {
  var k = typeof amount === &apos;number&apos; ? amount : 50,
    n_samples = 44100,
    curve = new Float32Array(n_samples),
    deg = Math.PI / 180,
    i = 0,
    x;
  for ( ; i &lt; n_samples; ++i ) {
    x = i * 2 / n_samples - 1;
    curve[i] = ( 3 + k ) * x * 20 * deg / ( Math.PI + k * Math.abs(x) );
  }
  return curve;
};
</pre>

<p>In the Voice-change-O-matic demo, we connect the <code>distortion</code> node up to the audio graph, then apply this to the distortion node when needed:</p>

<pre class="brush: js">source.connect(analyser);
analyser.connect(distortion);
distortion.connect(biquadFilter);

...

distortion.curve = makeDistortionCurve(400);
</pre>

<h3 id="Biquad_filter">Biquad filter</h3>

<p>The biquad filter has a number of options available inside it, and is created using the <a href="/ru/docs/Web/API/AudioContext/createBiquadFilter" title="Документация об этом ещё не написана; пожалуйста, поспособствуйте её написанию!"><code>AudioContext.createBiquadFilter</code></a> method:</p>

<pre class="brush: js">var biquadFilter = audioCtx.createBiquadFilter();
</pre>

<p>The particular option used in the Voice-change-o-matic demo is a lowshelf filter, which basically provides a bass boost to the sound:</p>

<pre class="brush: js">biquadFilter.type = &quot;lowshelf&quot;;
biquadFilter.frequency.value = 1000;
biquadFilter.gain.value = 25;
</pre>

<p>Here we specify the type of filter, a frequency value, and a gain value. In the case of a lowshelf filter, all frequencies below the specified frequency have their gain increased by 25 decibels.</p>

<h2 id="Other_things_inside_Web_Audio_API">Other things inside Web Audio API</h2>

<p>The Web Audio API can do many other things other than audio visualization and spatializations (e.g. panning sound). We will cover other options in further articles, as this one has become long enough already!</p>
